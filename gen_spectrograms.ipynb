{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating spectrograms from dataset\n",
    "\n",
    "reference: https://github.com/JoshWheeler08/DolphinAcoustics-Classifier\n",
    "\n",
    "This jupyter notebook is for everything related to generating spectrograms: including functions that fetch your input files, functions that generate spectrograms from an input wav form and saves them to a specified output file path.\n",
    "\n",
    "This code is fully os-independent and fs-independent, so you don't have to worry whether you are running on a windows os or IOS or linux, they will all work. But do make sure you set up a virtual environment correctly before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you installed all the dependencies already, ignore this block of code\n",
    "!pip3 install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code block is a Configurations/Settings block for the processing part of this file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input and download paths here.\n",
    "# the path module is operating sytem independent, it can create a posix path or a windows path.\n",
    "CLIPS_PATH = Path(\"wav-data\").resolve() # path to where you stored input wav-clips of dolphin sounds/whistles.\n",
    "SAVE_IMAGE_PATH = Path(\"img-data\") # path to where you want to save your images.\n",
    "\n",
    "# specify the classes of species there are in your input data here\n",
    "SPECIES = [\"common\", \"bottlenose\", \"melon-headed\"]\n",
    "\n",
    "# Configure parameters as you wish, currently they are equal to the default values in the function signature of save_spectrogram_image\n",
    "SAMPLING_RATE = 48000  # gemma's improved sampling rate\n",
    "FFT_NUM = 512  # fft number\n",
    "DPI = 96  # dots per inch of your screen, explained ahead why this is important\n",
    "MAX_FREQ = 22000\n",
    "MIN_FREQ = 3000\n",
    "IMAGE_SIZE = (413, 202) # (x, y)\n",
    "REF = np.max # save a function or a value that will act as a reference point in the amplitude_to_db function\n",
    "# https://librosa.org/doc/main/generated/librosa.amplitude_to_db.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code-block declares and defines functions for generating and saving spectrograms and finding clips in the file system.\n",
    "\n",
    "NB: matplotlib only works with real dimensions and not directly with pixels. So if you want to show or save an image of certain pixel you need to find out what dpi your screen uses.\n",
    "\n",
    "The following link allows you to detect the dpi of your screen:\n",
    "https://www.infobyip.com/detectmonitordpi.php\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrogram_image(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    image_name,\n",
    "    sampling_rate=48000,\n",
    "    n_fft=512,\n",
    "    dpi=96,\n",
    "    max_freq=22000, \n",
    "    min_freq=3000,  \n",
    "    img_size=(413, 202),\n",
    "    ref=np.max,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes in the above parameters and\n",
    "    generates a spectrogram from a given sample recording passed in by the \"input_path\" and\n",
    "    saves the spectrogram image in \"output_path\" with the name \"image_name\".\n",
    "\n",
    "    sampling_rate and n_fft variables are core variables that are used in the process of spectrogram generation.\n",
    "    dpi is the dots per inch of your screen.\n",
    "    max_freq and min_freq are used for cropping the image, read Dzhemma's report for a deeper understanding.\n",
    "    img_size should be the desired size of the saved images.\n",
    "    ref is the function/value you are using as a reference for fixing the zero in the amplitude_to_db function.\n",
    "\n",
    "    If ref is changed, you probably want to change the clim parameter as well, as it is currently designed for Ydb.\n",
    "\n",
    "    \"\"\"\n",
    "    f_step = sampling_rate / n_fft\n",
    "    min_bin = int(min_freq / f_step)\n",
    "    max_bin = int(max_freq / f_step)\n",
    "\n",
    "    # load wav file, apply short-time-fourier-transform, and crop the 2d array vertically, removing unwanted rows in the data.\n",
    "    y, sr = librosa.load(input_path, sr=sampling_rate)\n",
    "    Y = librosa.stft(y, n_fft=n_fft)\n",
    "    Y = Y[ min_bin:max_bin, :]  # Crop image vertically (frequency axis) from min_bin to max_bin\n",
    "\n",
    "    # change the amplitude from a linear scale to decibel scale (logarithmic)\n",
    "    Ydb = librosa.amplitude_to_db(\n",
    "        abs(Y), ref=ref\n",
    "    )\n",
    "    # initialise plot\n",
    "    fig = plt.figure(\n",
    "        frameon=False, figsize=(img_size[0] / dpi, img_size[1] / dpi), dpi=dpi\n",
    "    )\n",
    "    \n",
    "    # remove axes from the plot\n",
    "    ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    # plot the spectrogram on figure object in gray_r color mapping.\n",
    "    librosa.display.specshow(Ydb, cmap=\"gray_r\", sr=sr, x_axis=\"time\", y_axis=\"hz\", clim=[np.mean(Ydb),0])\n",
    "\n",
    "    # Save image at \"output_path/img_name.png\"\n",
    "    fig.savefig(os.path.join(output_path, str(image_name) + \".png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_all_wavfiles(root_path):\n",
    "    \"\"\"\n",
    "    File-system independent function for getting all the wav files from the given root directory.\n",
    "    Use walk function from the os module to get the root, dir and files in the given root_path.\n",
    "    Extract all paths to files and returns them in a generator so that it can be iterated over in a loop.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file_name in files:\n",
    "            path = os.path.join(root, file_name)\n",
    "            if path.endswith(\".wav\"):\n",
    "                yield(path)\n",
    "\n",
    "def create_storage_for_images(directory_to_store_images):\n",
    "    \"\"\"\n",
    "    Create storage for images with shutil and os libraries.\n",
    "    \"\"\"\n",
    "    if os.path.exists(directory_to_store_images):\n",
    "        shutil.rmtree(directory_to_store_images)\n",
    "    os.makedirs(directory_to_store_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executable code that loops through all the input wav-files and saves generated spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the count as the image name so that we can check if any of them were left out after the execution\n",
    "count = 0\n",
    "# make sure your input file directory has folders under the name of the species declared in the species array.\n",
    "for specie in SPECIES:\n",
    "    curr_input_dir = os.path.join(CLIPS_PATH, specie)\n",
    "    curr_output_dir = os.path.join(SAVE_IMAGE_PATH, specie)\n",
    "    create_storage_for_images(curr_output_dir)\n",
    "    for clip_path in get_all_wavfiles(curr_input_dir):\n",
    "        if not clip_path.endswith(\".wav\"):  # defensive code, steps over any files that does not have the .wav ending\n",
    "            continue\n",
    "        # print(\"saving...\", clip_path)\n",
    "        save_spectrogram_image(\n",
    "            clip_path,\n",
    "            curr_output_dir,\n",
    "            count,\n",
    "            SAMPLING_RATE,\n",
    "            FFT_NUM,\n",
    "            DPI,\n",
    "            MAX_FREQ, \n",
    "            MIN_FREQ, \n",
    "            IMAGE_SIZE,\n",
    "            REF,\n",
    "        )\n",
    "        # print(\"saved!\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset of the generated spectrograms into training and testing folders.\n",
    "This automatic method of splitting data sets can help you generate datasets ready for training in the whistle_classifier file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_folder(path):\n",
    "    # can't reuse get_all_wavfiles here because we need to return an array of all the files\n",
    "    # to count how much data we have\n",
    "    files = os.listdir(path)\n",
    "    return np.asarray(files)\n",
    "\n",
    "def generate_training_and_testing_datasets(path_to_data, path_to_test_data, train_test_ratio=0.6):\n",
    "    \"\"\"\n",
    "    Takes in a path_to_data which is the input path of where all the spectrogram images are stored, and\n",
    "    path_to_test_data which is where you want to move images into the test folders.\n",
    "    The train_test_ratio specifies the the ratio of the split.\n",
    "    \"\"\"\n",
    "    _, dirs, _ = next(os.walk(path_to_data))\n",
    "\n",
    "    # counts training data per species\n",
    "    counter_per_species = np.zeros((len(dirs)))\n",
    "    for i in range(len(dirs)):\n",
    "        path = os.path.join(path_to_data, dirs[i])\n",
    "        files = get_files_from_folder(path)\n",
    "        counter_per_species[i] = len(files)\n",
    "\n",
    "    test_counter = np.round(counter_per_species * (1 - train_test_ratio))\n",
    "\n",
    "    # transfers files\n",
    "    for i in range(len(dirs)):\n",
    "        path_to_original = os.path.join(path_to_data, dirs[i])\n",
    "        path_to_save = os.path.join(path_to_test_data, dirs[i])\n",
    "\n",
    "        # creates dir\n",
    "        if not os.path.exists(path_to_save):\n",
    "            os.makedirs(path_to_save)\n",
    "\n",
    "        # moves data\n",
    "        files = get_files_from_folder(path_to_original)\n",
    "        for j in range(int(test_counter[i])):\n",
    "            dst = os.path.join(path_to_save, files[j])\n",
    "            src = os.path.join(path_to_original, files[j])\n",
    "            shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "executable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = Path(\"test-data\")\n",
    "TRAIN_TEST_RATIO = 0.7\n",
    "generate_training_and_testing_datasets(SAVE_IMAGE_PATH, TEST_DATA, TRAIN_TEST_RATIO)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "039371b77139fa2b9bd518eca1b62ab7d3ddf0505b49296a18c58435d742de7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('dolphins': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
